{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import konlpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Comptuer scientists are just nerds',\n",
    "    'Social scientists are attractive',\n",
    "    'Are nerds attractive',\n",
    "    'Probably not',\n",
    "    'THE BOOM shall rise'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comptuer': 3,\n",
       " 'scientists': 9,\n",
       " 'are': 0,\n",
       " 'just': 4,\n",
       " 'nerds': 5,\n",
       " 'social': 11,\n",
       " 'attractive': 1,\n",
       " 'probably': 7,\n",
       " 'not': 6,\n",
       " 'the': 12,\n",
       " 'boom': 2,\n",
       " 'shall': 10,\n",
       " 'rise': 8}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'Comptuer scientists are just nerds.',\n",
    "    'Social scientists are attractive.',\n",
    "    'Are nerds attractive?',\n",
    "    'Probably not.',\n",
    "    'THE BOOM shall rise'\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictdf = pd.DataFrame.from_dict(vect.vocabulary_, orient='index').reset_index()\n",
    "dictdf.columns = ['word', 'num']\n",
    "\n",
    "dictdf = dictdf.sort_values('num').reset_index(drop=True)\n",
    "dictdf.to_csv('./data/20210206dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['Economists are nerds']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['Social science boom will rise']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['I majored in management engineering']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comptuer': 2,\n",
       " 'scientists': 8,\n",
       " 'just': 3,\n",
       " 'nerds': 4,\n",
       " 'social': 10,\n",
       " 'attractive': 0,\n",
       " 'probably': 6,\n",
       " 'not': 5,\n",
       " 'boom': 1,\n",
       " 'shall': 9,\n",
       " 'rise': 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Stop Words\n",
    "vect2 = CountVectorizer(stop_words=[\"are\",\"the\"]).fit(corpus)\n",
    "vect2.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중요하지 않은 (혹은 noise들을) 어휘들을 어떻게 제거할 것인가\n",
    "1. 형태소분석기(tokenizer, pos-tagger)를 사용하면서 명사/동사/형용사 중 특정 pos만 추출\n",
    "2. Stop Words를 설정 (제외할 불용어)\n",
    "3. 형태소분석기를 사용 후 Stop Words 제거 (1+2)\n",
    "\n",
    "### Why Tf-IDF?\n",
    "1. 위의 전처리 스텝을 자동화하여 불용어 여부(binary) + 사전포함여부(binary) 판별을 자동화하자\n",
    "2. \"가중치\" 개념을 부여하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfVect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34582166, 0.        , 0.        , 0.51637397, 0.51637397,\n",
       "        0.41660727, 0.        , 0.        , 0.        , 0.41660727,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.40382593, 0.48648432, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.48648432,\n",
       "        0.        , 0.60298477, 0.        ],\n",
       "       [0.50620441, 0.60981846, 0.        , 0.        , 0.        ,\n",
       "        0.60981846, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.70710678, 0.70710678, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.5       , 0.        ,\n",
       "        0.5       , 0.        , 0.5       ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv_result = TfVect.fit_transform(corpus)\n",
    "tfidv_result.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.34232466, 0.42911125, 0.        , 0.        ],\n",
       "       [0.34232466, 1.        , 0.50108558, 0.        , 0.        ],\n",
       "       [0.42911125, 0.50108558, 1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_distance = tfidv_result * tfidv_result.T\n",
    "doc_distance.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comptuer': 3,\n",
       " 'scientists': 9,\n",
       " 'are': 0,\n",
       " 'just': 4,\n",
       " 'nerds': 5,\n",
       " 'social': 11,\n",
       " 'attractive': 1,\n",
       " 'probably': 7,\n",
       " 'not': 6,\n",
       " 'the': 12,\n",
       " 'boom': 2,\n",
       " 'shall': 10,\n",
       " 'rise': 8}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vect = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n",
    "unigram_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comptuer scientists': 4,\n",
       " 'scientists are': 8,\n",
       " 'are just': 1,\n",
       " 'just nerds': 5,\n",
       " 'social scientists': 10,\n",
       " 'are attractive': 0,\n",
       " 'are nerds': 2,\n",
       " 'nerds attractive': 6,\n",
       " 'probably not': 7,\n",
       " 'the boom': 11,\n",
       " 'boom shall': 3,\n",
       " 'shall rise': 9}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vect = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "bigram_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comptuer scientists are': 3,\n",
       " 'scientists are just': 5,\n",
       " 'are just nerds': 0,\n",
       " 'social scientists are': 6,\n",
       " 'scientists are attractive': 4,\n",
       " 'are nerds attractive': 1,\n",
       " 'the boom shall': 7,\n",
       " 'boom shall rise': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_vect = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
    "trigram_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comptuer': 10,\n",
       " 'scientists': 21,\n",
       " 'are': 0,\n",
       " 'just': 13,\n",
       " 'nerds': 15,\n",
       " 'comptuer scientists': 11,\n",
       " 'scientists are': 22,\n",
       " 'are just': 2,\n",
       " 'just nerds': 14,\n",
       " 'comptuer scientists are': 12,\n",
       " 'scientists are just': 24,\n",
       " 'are just nerds': 3,\n",
       " 'social': 27,\n",
       " 'attractive': 6,\n",
       " 'social scientists': 28,\n",
       " 'are attractive': 1,\n",
       " 'social scientists are': 29,\n",
       " 'scientists are attractive': 23,\n",
       " 'are nerds': 4,\n",
       " 'nerds attractive': 16,\n",
       " 'are nerds attractive': 5,\n",
       " 'probably': 18,\n",
       " 'not': 17,\n",
       " 'probably not': 19,\n",
       " 'the': 30,\n",
       " 'boom': 7,\n",
       " 'shall': 25,\n",
       " 'rise': 20,\n",
       " 'the boom': 31,\n",
       " 'boom shall': 8,\n",
       " 'shall rise': 26,\n",
       " 'the boom shall': 32,\n",
       " 'boom shall rise': 9}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_to_trigram_vect = CountVectorizer(ngram_range=(1, 3)).fit(corpus)\n",
    "uni_to_trigram_vect.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
